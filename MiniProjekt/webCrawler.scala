/* 
Seite herunterladen
Funktion, die eine URL nimmt und den HTML-Inhalt als String zurückgibt.

Links extrahieren
Funktion, die aus einem HTML-String alle Hyperlinks extrahiert und als Liste von URLs zurückgibt.

Rekursives Crawling
Funktion, die ab einer Start-URL rekursiv alle gefundenen Links besucht (bis zur gewünschten Tiefe, Seitenanzahl oder Timeout).

Baumstruktur aufbauen
Funktion, die die besuchten Seiten und deren ausgehende Links als Baum oder Map speichert.

Ergebnis speichern
Funktion, die die Baumstruktur in eine Datei schreibt (z.B. als JSON oder Text).

Crawl-Limits prüfen
Funktionen, die prüfen, ob maximale Tiefe, Seitenanzahl oder Zeit überschritten sind.
*/